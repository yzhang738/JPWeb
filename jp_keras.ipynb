{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses keras package to design LSTM neural networks to test on the etl data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set values for \"credentias\", \"bucket_name\" and \"endpoint\" on the cloud\n",
    "credentials = {\n",
    "}\n",
    "bucket_name = \"\"\n",
    "endpoint = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from botocore.client import Config\n",
    "from boto3 import ibm_boto3\n",
    "import time\n",
    "\n",
    "# Create client \n",
    "client = ibm_boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id = credentials[\"cos_hmac_keys\"]['access_key_id'],\n",
    "    aws_secret_access_key = credentials[\"cos_hmac_keys\"][\"secret_access_key\"],\n",
    "    endpoint_url=endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.download_file(bucket_name,'good_result.csv', 'good_result.csv')\n",
    "client.download_file(bucket_name,'faulty_result.csv', 'faulty_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_good = pd.read_csv('good_result.csv', engine='python', header=None)\n",
    "df_good.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_good.loc[df_good[1] == 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_faulty = pd.read_csv('faulty_result.csv', engine='python', header=None)\n",
    "df_faulty.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_records(df,file_id):\n",
    "    return np.array(df.sort_values(by=0, ascending=True).loc[df[1] == file_id].drop(0,1).drop(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "good_sample = get_records(df_good,100)\n",
    "faulty_sample = get_records(df_faulty,105)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "size = len(good_sample)\n",
    "ax.plot(range(0,size), good_sample[:,0], '-', color='red', animated = True, linewidth=1)\n",
    "ax.plot(range(0,size), good_sample[:,1], '-', color='blue', animated = True, linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "size = len(faulty_sample)\n",
    "ax.plot(range(0,size), faulty_sample[:,1], '-', color='red', animated = True, linewidth=1)\n",
    "ax.plot(range(0,size), faulty_sample[:,0], '-', color='blue', animated = True, linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 100\n",
    "dim = 2\n",
    "lossHistory = LossHistory()\n",
    "\n",
    "# design network\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50,input_shape=(timesteps,dim),return_sequences=True))\n",
    "model.add(Dense(2))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "def train(data):\n",
    "    model.fit(data, data, epochs=20, batch_size=72, validation_data=(data, data), verbose=1, shuffle=False,callbacks=[lossHistory])\n",
    "\n",
    "def score(data):\n",
    "    yhat =  model.predict(data)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# reduce number of threads\n",
    "os.environ['TF_NUM_INTEROP_THREADS'] = '1'\n",
    "os.environ['TF_NUM_INTRAOP_THREADS'] = '1'\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trimmed_records(df,file_id):\n",
    "    records = get_records(df,file_id) \n",
    "    samples = len(records)\n",
    "    trim = samples % 100\n",
    "    records_trimmed = records[:samples-trim]\n",
    "    records_trimmed.shape = (int((samples-trim)/timesteps),timesteps,dim)\n",
    "    return records_trimmed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(df_good.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ids = pd.unique(df_good.iloc[:,1])\n",
    "start = time.time()\n",
    "for file_id in file_ids:\n",
    "    recording_trimmed = create_trimmed_records(df_good,file_id)\n",
    "    print(\"Staring training on %s\" % (file_id))\n",
    "    train(recording_trimmed)\n",
    "    print(\"Finished training on %s after %s seconds\" % (file_id,time.time()-start))\n",
    "\n",
    "print(\"Finished job on after %s seconds\" % (time.time()-start))\n",
    "healthy_losses = lossHistory.losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "size = len(healthy_losses)\n",
    "plt.ylim(0,0.008)\n",
    "ax.plot(range(0,size), healthy_losses, '-', color='blue', animated = True, linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ids = pyspark.sql('select distinct _c1 from df_good').rdd.map(lambda row : row._c1).collect()\n",
    "start = time.time()\n",
    "for file_id in [105]:\n",
    "    recording_trimmed = create_trimmed_records(df_faulty,file_id)\n",
    "    print(\"Staring training on %s\" % (file_id))\n",
    "    train(recording_trimmed)\n",
    "    print(\"Finished training on %s after %s seconds\" % (file_id,time.time()-start))\n",
    "\n",
    "print(\"Finished job on after %s seconds\" % (time.time()-start))\n",
    "faulty_losses = lossHistory.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ids = pd.unique(df_faulty.iloc[:,1])\n",
    "start = time.time()\n",
    "for file_id in file_ids:\n",
    "    recording_trimmed = create_trimmed_records(df_faulty,file_id)\n",
    "    print(\"Staring training on %s\" % (file_id))\n",
    "    train(recording_trimmed)\n",
    "    print(\"Finished training on %s after %s seconds\" % (file_id,time.time()-start))\n",
    "\n",
    "print(\"Finished job on after %s seconds\" % (time.time()-start))\n",
    "faulty_losses = lossHistory.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(num=None, figsize=(14, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "size = len(healthy_losses+faulty_losses)\n",
    "plt.ylim(0,0.008)\n",
    "ax.plot(range(0,size), healthy_losses+faulty_losses, '-', color='blue', animated = True, linewidth=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "3200cc2825679299acbc482d7e6aa73e3fd1a97ab8a972209aa62dca63dbe754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
